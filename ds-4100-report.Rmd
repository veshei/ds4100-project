---
output:
  word_document: default
  pdf_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rvest)
library(tidyverse)
library(RSQLite)
library(fTrading)
library(ggplot2)
library(quantmod)
db <- dbConnect(SQLite(), dbname = "ds4100_project_db.sqlite")
```

Problem Description
-----

  Stock markets serve an important function in any society, they play a pivotal role in the growth of the industry and commerce of the country and ultimately plays a large effect on the economy of a country. For many individual investors, investing in the stock market can seem like a daunting task. With the amount of data and advice available online, it becomes difficult for the average consumer to determine important factors and considerations when investing in stocks. Additionally, the large number of technical indicators ranging from simple moving averages to the stochastic oscillator can seem like foreign words to those not trained or educated in the financial field. 
  
  However, because investing smartly in stocks, is one of the key paths of wealth creation and is an important investment avenue that deters too many average consumers. Our hopes with this project is to utilize individual stock data from the Standard & Poor’s 500, a stock market index that tracks the top 500 United States corporations, historical market prices, and related technical indicators for each stock, to generate predictive analysis of whether to buy or sell a specific stock. We hope that this project helps everyday investors make more data-driven choices in determining which stocks to invest in that would beat the returns from the S&P 500 index. Because our algorithm takes into account the various different technical indicators and stock prices, this removes a lot of the calculations and understandings on the part of the average investor. For the more savvy investors, we hope our machine learning algorithms still serve as a strong basis for their purchase decisions. 

Rationale for Data Collection
-----
  The S&P 500 is an American stock market index based on the market capitalizations of 500 large companies that have stock in common with those listed on NYSE, NASDAQ, or the CBOE BZX Exchange. We chose the S&P 500 for this project because we believed that the diverse constituency and weighting methodology it utilizes made it one of the best representations of the U.S. stock market. These features meant stocks in the S&P 500 would serve as a strong baseline of data for us to build our predictive models on. 
  
  We chose stocks in the S&P 500 over other well regarded indices like NASDAQ Composite index or the Dow Jones Industrial Average because the sheer number of corporations tracked by the S&P 500 outclasses the other similar indices. We believed that feeding more training data into our machine learning algorithms would only make our predictive analytics smarter and more accurate. Additionally, most of the stocks in the S&P 500 have a large amount of historical stock data available because the S&P 500 tracks large-cap firms, which tend to have a longer history on the public stock market in order to develop to that point. We also chose to limit our finding to the past 5 years of financial data to keep the size and scope of the data more manageable while also providing enough breadth for outliers to have any outsized effect on our predictions. 

Process Justifications and Technical Choices
---
  The R library we used to get the S&P 500 data is Quantmod. We first scraped the S&P 500 Wikipedia page to retrieve a list of firm stock symbols for us to for loop through. Then, based on this list of symbols, we called the Quantmod function getSymbols to create 500 dataframes of each stock. Quantmod returned a dataset of clean variables that provided stock prices per day up to current day and spanning 20+ years of historical data. We then utilized the R library, fTrading, to calculate different technical indicators like SMA, EWMA, MOM, and more. We also wrote several of our own functions for technical indicators not covered by fTrading, such as percent values, percent change, and buy/sell indicators. We column binded all of these technical indicators to the stock data of each S&P 500 firm. We then created 500 SQLite tables that properly mapped our table requirements out and were in third normal form. We wrote the data frames we had generated to these SQLite tables.
  
  Prior to piping in our data to the machine learning algorithms, we also made sure to remove any outsized outliers and normalize categories like price and volume. The data also had to be cleaned for NAs, 0 values, and 1 values that were padding generated from the calculation of certain technical indicators such as rolling averages (SMA, EWMA) and indicators requiring a specific period of inital data such as RSI. The team decided to remove these padding values as there were more than enough data point were the team did not have to be stingy about retaining data. We determined outliers by plotting a histogram of the density of the different lengths of each stock table and removed tables with less than 1000 values.
  
  The data was then modeled using several different methodologies in order to search for promising routes. This involved the 2 general response variables of an observation's future percent change in price and an observations buy classification which was designated if a stock had a price higher in the future than now. The number of days that these response variables "looked ahead" was broken down into short term, medium term, and long term. There are a few different definitions for this, but the team defined short term as 20 trading days (1 month), medium term as 60 trading days (3 months), and 240 trading days (1 year). The team intended on utilizing linear regression in order to predict the stock price, while logistic regression and knn would be used to classify the observations.
  
  The dataset was then modeled with the entire universe of observations, which was around 600,000 observations spanning 495+ stocks on 10 features and on a specific stock level, which was generally around 1,000 observations. The team wanted to explore the different model behaviors on the data size, such as whether the models would be more robust when fed the universe of data and if there was a set of uniform behaviors demonstrated by the indicators. These datasets would then be split into a 70-30 ratio for training and testing with random shuffling utilized. Random shuffling was used due to the focus on analyzing the technical indicator values rather than the strict date progression.
  
  Before the team began, the distributions of the response variables were analyzed to provide insight into any values that ocurred. The classification responses demonstrated a larger amount of buys over the sells, which made sense as the dataset was pulled from a bull market. What was interesting was that the short term time frame had only slight advantage in the number of buys, but as the time period increased, there were more and more buy classifications. The team quickly realized this was due to mean-reversion in a bull market, where short term shock could influence the direction of the stock, but as time went on the performers that went against the upward trend would tend to follow the general population and move upwards as well. The percentage change in future prices was also plotted and the team found that these distirbutions were fairly normal with the exception of a few outliers. It was interesting to note here that the distributions were mean-centered around 0 with a slight left-skew, which the team attributes to the bull market again.
  
  The team first began with regression on the universal dataset of stocks, which was suspected to be difficult to model simply due to different sectors included in the dataset and the unlimited range of real world results. However, the results were extremely poor with R^2 values of under 0.05 repeatedly ocurring for the short term linear regression. There was a general trend of increasing R^2 values for longer time periods when looking at the medium and long term variables. A plot of the actual to predicted values showed that there was no apparent correlation.The team quickly realized there were a few factors at play, such as all three models being dominated by different technical indicators. This shows that none of the technical indicators for linear regression were proving to be particularly robust or consistently impactful. The team saw that the models were often dominated by a single variable that had a coefficient with one or two orders of magnitude above the rest, which further reinforces the previous point.
  
  The team then moved onto logistic regression, which was suspected to be much more simpler to identify due to the binary classification. The results showed that the short term model had an accuracy rate of 65%, which increased as expected due to the mean-reversion effect of the bull market for the medium and long term reponses up to 75%. This showed that classification was promising, but upon digging into the data, the team realized that the models had an extreme issue of specificity. Nearly all of the predictions were to buy, which meant that the accuracy rate simply depended on bull market characteristics rather than the technical analysis indicators.
  
  A final model was then evaluated for the universal dataset, but attempting to use 11 nearest neighbors on 300,000 training observations to classify over a 100,000 test observations was demonstrating to take far too long. An hour into the algorithm, the team decided to cancel it and move onto other analysis.
  
  The previous models on the varying time frames were run again on indiidual stocks, which the models were able to predict slighly more accurately. However, the same issues were again encountered in the form of a single feature dominating the prediction and near zero R^2 values for linear regression and very high specificity for logistic regression. The team attempted to try kNN again with a k of 11 and it showed to be very robust with accuracy values of 65% for short term periods going up to 85% for long term periods. The team analyzed the model for the specificity issue and realized that there was a bias for false positives, but there was a greater range of observations classified as sells, which is what improved the accuracy rate. The bias towards classifying as buys was also determined to be acceptable as the neighbors of a knn would tend to be buys. 

Project Issues and Resolution
-----
  We ran into several issues during the course of this project, with the biggest barriers being data sourcing, amount of data, data transformations, and file size. Firstly, our team had difficult finding free sources of historical and current stock data in the available R library packages and API’s we found online. While we were initially intrigued by libraries like Quandl and AlphaVantage, both libraries either don’t release the prime data (like U.S. stock data) to free customers, or throttle the amount of calls a free user can make. Both of these limitations made these options unsustainable for our purposes as we needed needed to retrieve 5 years worth of data from 500 corporations.
  
  We then experimented with web scraping the data from Yahoo Finance. However we ran into issues of consistent data and throttled usage as Yahoo Finance blocks users who are web scraping a large amount of data. Because of how variable and fast changing web pages can be, we ran into many issues trying to consolidate and pull the data via web scraping. Additionally, this was a very slow process as we needed to put in system sleeps to prevent us from being blocked from the site.
  
  Ultimately, we were able to resolve our data collection issues by using the R library Quantmod, which provided free historical stock data on all the US stocks. Because this was an R library package and not an api for us to pull on, the built in functions and data made this process much faster and seamless than the two options mentioned above. 
  
  We encountered some issues with the amount of data to clean and transform. Firstly we had naming issues in our database as there was no direct way to loop through the 500 tables other than looping through the list of S&P stock names, meaning we had to utilize the get() and assign() functions so that we were calling the dataframe and not the string name. There is some efficiency issues in generating 5 years worth of data for 500 firms. Additionally, the sheer scope of this data made it harder for us to check for accuracy in our calculations. We worked through this issue by implementing all the data cleaning and transformations on a small subset of the data first and then scaling up by utilizing for loops. Also, because we only needed to transform, clean, and write the data to our database once, this was not a process that had to be repeated many times over. 
  
  Finally, we had issues with the large file size of our SQLite database file. Our team used Github for version control and our database file ended up being over the allowable file size of 100 mb. We were able to work through this issue by implementing Github Large File Storage (LFS). However, this was not a barrier we expected to encounter and learning to use Github LFS and rolling back our commits took up more time than expected. 

Input and Output from R Code
-----
An example of a stock table we generated. 
```{r, example_data, message=FALSE, echo=TRUE, warning=FALSE}
head(dbReadTable(conn = db, "AAPL"))
```

An example of what an analyst may look at when viewing a stock. Comparing 50 day to 200 day chart series. 
```{r, stock_charts, message=FALSE, echo=TRUE, warning=FALSE}
#Stock graph for AAPL
#Indicators shown are Momentum, WMA, SMA, MACD, and EVWMA
#Display 200 day view starting from October 16th
getSymbols("AAPL", from='2018-10-16', to='2019-04-16')
chartSeries(AAPL, TA = "addMomentum();addWMA();addSMA();addMACD();addEVWMA()", theme="white",major.ticks='months',subset='first 28 weeks')
#Displays 50 day view starting from Dec 17th
getSymbols("AAPL", from='2018-12-16', to='2019-04-16')
chartSeries(AAPL, TA = "addMomentum();addWMA();addSMA();addMACD();addEVWMA()", theme="white",major.ticks='months',subset='first 50 days')
```

Insights and Potential Future work
-----
  Potential future work we foresee for this project is to increase the number of stocks or the number of years we feed into our predictive algorithms. We hope that with more data, we’d be able to increase the accuracy of our data. We could also expand the number of technical indicators used to see if that would improve the accuracy of our predictions. Another approach we could explore is to use different baseline data such as social media, fundamentals, or the economy to see if those are better predictors of the US stock market. We could also test out more predictive algorithms to determine the most accurate algorithm.  

Works Cited
-----
Chalabi, Yohan. "FTrading v3010.78." FTrading Package | R Documentation,   www.rdocumentation.org/packages/fTrading/versions/3010.78.

Miller, Curtis. “An Introduction to Stock Market Data Analysis with R (Part 1).” Curtis Miller's Personal Website, 28 Mar. 2017, ntguardian.wordpress.com/2017/03/27/introduction-stock-market-data-r-1/.

Wikipedia. "List of S&P 500 Companies." Wikipedia, Wikimedia Foundation, 13 Apr. 2019, en.wikipedia.org/wiki/List_of_S%26P_500_companies.


